{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Поиск, подготовка и анализ данных\n",
    "\n",
    "### Автор: Ракитин Виталий, BD-21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from HTMLParser import HTMLParser\n",
    "from lxml import html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Откачиваем данные с сайта otvet.mail.ru\n",
    "\n",
    "Несложно заметить, что на сайте otvet.mail.ru все вопросы пронумерованы (по id), соответсвтенно и откачивать данные межно простым обходом по номерам. Ограничимся первыми 10млн. вопросами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# headaers for http request\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:45.0) Gecko/20100101 Firefox/45.0',\n",
    "    'Accept' : 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Encoding' : 'gzip,deflate,sdch',\n",
    "    'Accept-Language' : 'en-US,en;q=0.8',\n",
    "}\n",
    "\n",
    "html = HTMLParser()\n",
    "PARSE_FILE = \"data.csv\"\n",
    "READY_STR = \"READY: {0} : {1} of {2} --- {3:.2f}%;\"\n",
    "\n",
    "def get(num, amount = 10):\n",
    "    question = None\n",
    "    answers = None\n",
    "    try:\n",
    "        request = requests.get('https://otvet.mail.ru/question/' + str(num), headers = HEADERS) \n",
    "        question = re.findall(u'<title>Ответы@Mail.Ru: (.*?)</title>', request.text)[0]\n",
    "        question = html.unescape(question)\n",
    "        question = question.encode('utf-8')\n",
    "        \n",
    "        answers = re.findall(u'<div class=\"a--atext atext\" itemprop=\"text\">(.*?)</div>', request.text)\n",
    "        answers = map(lambda x: re.sub(u'<br><br>','\\n', x), answers)\n",
    "        answers = map(lambda x: re.sub(u'<.*>','\\n', x), answers)\n",
    "        answers = map(lambda x: re.sub(u'\\n',' ', x), answers)\n",
    "        answers = map(lambda x: html.unescape(x), answers)\n",
    "        answers = map(lambda x: x.encode('utf-8'), answers)\n",
    "        answers = [ans for ans in answers if len(ans) > 1]\n",
    "        answers = answers[0:4]\n",
    "\n",
    "    except:\n",
    "        pass #Page does not exist\n",
    "    return question, answers\n",
    "\n",
    "def save(question, answers, question_id, file = PARSE_FILE):\n",
    "    ''' save qusetion in file '''\n",
    "    if question != None and answers != None:\n",
    "        with open(file, \"a\") as file:\n",
    "            \n",
    "            file.write(str(question_id))\n",
    "            file.write(\"\\t\\t\")\n",
    "            file.write(question)\n",
    "            for answer in answers:\n",
    "                file.write(\"\\t\\t\")\n",
    "                file.write(answer)\n",
    "           \n",
    "            file.write(\"\\n\")\n",
    "\n",
    "def download(first, last, file = PARSE_FILE, frequency = 1000):\n",
    "    good = 0\n",
    "    bad = 0\n",
    "    pages = xrange(first, last)\n",
    "    for page_num in pages:\n",
    "        question, answers = get(page_num)\n",
    "        if question != None: \n",
    "            save(question, answers, page_num, file)\n",
    "            good += 1\n",
    "        else:\n",
    "            bad += 1\n",
    "\n",
    "        if (bad + good) % frequency == 0:\n",
    "            print READY_STR.format(good, bad, last - first, \n",
    "                                   float(good+bad)/float(last-first)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Vitaliy/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:3: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "downloaded = True\n",
    "if downloaded:\n",
    "    itterator = pd.read_csv(\"parse.csv\",sep = \"\\t\\t\", header=None, chunksize=10000)\n",
    "    for table in itterator:\n",
    "        break\n",
    "else:\n",
    "    download(0, 10000000, \"parse.csv\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Какие документы нужны для получения загранпасп...</td>\n",
       "      <td>Список необходимых документов для получения за...</td>\n",
       "      <td># Оригинал гражданского паспоpта для подачи до...</td>\n",
       "      <td>два документа достоинством 100 долларов.</td>\n",
       "      <td>Все</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Где в Подмосковье или в Москве можно показатьс...</td>\n",
       "      <td>Можно в Солярисе:</td>\n",
       "      <td>Привет из 2017 года</td>\n",
       "      <td>Можно в Солярисе.</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>Как называется столица Албании?</td>\n",
       "      <td>Тирана.</td>\n",
       "      <td>Тирана!</td>\n",
       "      <td>Тирана</td>\n",
       "      <td>Тирана.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Хочу пострелять в тире. Где в Москве это можно...</td>\n",
       "      <td>Попробуй здесь:</td>\n",
       "      <td>Я тоже хочу! Возьми меня с собой!</td>\n",
       "      <td>на Октябрьском поле есть какой-то хороший тир ...</td>\n",
       "      <td>На Октябрьском</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>Посоветуйте молодых и недорогих, но прикольных...</td>\n",
       "      <td>Что еще есть?  А по мне, все равно выгоднее дв...</td>\n",
       "      <td>Слава Зверев :) Ну оочень молодой</td>\n",
       "      <td>Евгения Островская -  P.S. это не моя родствен...</td>\n",
       "      <td>Зайцев =))</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0                                                  1  \\\n",
       "0  2  Какие документы нужны для получения загранпасп...   \n",
       "1  3  Где в Подмосковье или в Москве можно показатьс...   \n",
       "2  4                   Как называется столица Албании?    \n",
       "3  5  Хочу пострелять в тире. Где в Москве это можно...   \n",
       "4  6  Посоветуйте молодых и недорогих, но прикольных...   \n",
       "\n",
       "                                                   2  \\\n",
       "0  Список необходимых документов для получения за...   \n",
       "1                                Можно в Солярисе:     \n",
       "2                                           Тирана.    \n",
       "3                                  Попробуй здесь:     \n",
       "4  Что еще есть?  А по мне, все равно выгоднее дв...   \n",
       "\n",
       "                                                   3  \\\n",
       "0  # Оригинал гражданского паспоpта для подачи до...   \n",
       "1                                Привет из 2017 года   \n",
       "2                                            Тирана!   \n",
       "3                 Я тоже хочу! Возьми меня с собой!    \n",
       "4                  Слава Зверев :) Ну оочень молодой   \n",
       "\n",
       "                                                   4               5  \n",
       "0          два документа достоинством 100 долларов.              Все  \n",
       "1                                  Можно в Солярисе.            None  \n",
       "2                                             Тирана         Тирана.  \n",
       "3  на Октябрьском поле есть какой-то хороший тир ...  На Октябрьском  \n",
       "4  Евгения Островская -  P.S. это не моя родствен...      Зайцев =))  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Анализ откаченных данных\n",
    "\n",
    "1. Попробуем посмореть, что из себя представляют откаченные данные на небольшой выборке.\n",
    "\n",
    "2. Построим словарь из используемых слов.\n",
    "\n",
    "3. Токенизируем нашу выборку с помощью метода one-hot-encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class one_hot_tokenizer():\n",
    "    def __init__(self, is_stem = False):\n",
    "        ''' \n",
    "            * self.vocabulary : {word : id}\n",
    "            * self.back_vocabulary : {id : words}\n",
    "            * self.words_count_dict : {word : word_counter}\n",
    "            * self.words_counter - all words counter (int)\n",
    "        Input: \n",
    "            * is_stem : to do stemming or not (False default)\n",
    "        '''\n",
    "        self.vocabulary = {}\n",
    "        self.back_vocabulary = {}\n",
    "        self.words_count_dict = {}\n",
    "        \n",
    "        stop_words = stopwords.words(u'russian')\n",
    "        self.stem = SnowballStemmer(\"russian\")\n",
    "        self.stop_words = [self.stem.stem(i) for i in stop_words]\n",
    "        self.words_counter = 0\n",
    "        self.is_stem = is_stem\n",
    "        \n",
    "    def tokenize_me(self, text):\n",
    "        ''' \n",
    "        Токенизируем предложение \n",
    "            * очищаем от мусора\n",
    "            * удаляем стоп-слова\n",
    "            * оставляем только русские буквы (слова) \n",
    "        Input:\n",
    "            * text : to clean\n",
    "        Output: \n",
    "            * tokens\n",
    "        '''\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(u'[а-яА-Я][а-яА-Я]*', text)\n",
    "\n",
    "        #stemming\n",
    "        if self.is_stem:\n",
    "            tokens = [self.stem.stem(i) for i in tokens]\n",
    "\n",
    "\n",
    "        #deleting stop_words\n",
    "        new_tokens = [i for i in tokens if i not in self.stop_words]\n",
    "        if len(new_tokens) < 2:\n",
    "            new_tokens = tokens\n",
    "\n",
    "        return new_tokens\n",
    "\n",
    "\n",
    "    def create_word(self, word, build_voc = True):\n",
    "        ''' \n",
    "            1. Дополняем словарь;\n",
    "            2. Узнаём индекс слова;\n",
    "        Input:\n",
    "            * word \n",
    "            * build_voc - to add word in self.vocabulary\n",
    "        Output:\n",
    "            * word_id\n",
    "        '''\n",
    "\n",
    "        if word in self.vocabulary:\n",
    "            word_id = self.vocabulary[word]\n",
    "            if build_voc:\n",
    "                self.words_count_dict[word_id] += 1\n",
    "                self.words_counter += 1\n",
    "        elif build_voc:\n",
    "            word_id = len(self.vocabulary)\n",
    "            self.vocabulary[word] = word_id\n",
    "            self.back_vocabulary[word_id] = word\n",
    "            self.words_count_dict[word_id] = 1\n",
    "            self.words_counter += 1 \n",
    "        else: \n",
    "            word_id = None\n",
    "        return word_id\n",
    "    def create_sentance(self, sentance, build_voc = True):\n",
    "        ''' \n",
    "            Токенизируем и индексируем предложение \n",
    "        Input:\n",
    "            * sentance\n",
    "            * build_voc - to add words in self.vocabulary\n",
    "        Output:\n",
    "            * tokens of the sentance (list)\n",
    "        '''\n",
    "        if sentance is not None and len(sentance) > 0:\n",
    "            try:\n",
    "                sentance = sentance.decode('utf-8')\n",
    "            except:\n",
    "                pass\n",
    "            sentance = self.tokenize_me(sentance)\n",
    "            sentance = [self.create_word(word, build_voc) for word in sentance]\n",
    "            return sentance\n",
    "        else: \n",
    "            return None\n",
    "\n",
    "    def fit(self, table, build_voc = True):\n",
    "        ''' \n",
    "            Токенизируем данные и строим словарь\n",
    "        Input:\n",
    "            * table : data (pandas)\n",
    "            * build_voc : to add words in self.vocabulary\n",
    "        Output:\n",
    "            * questions : tokenized questions (dict) \n",
    "            * answers : tokenized answers (dict) \n",
    "            * bad_sent : bad sentances (list) \n",
    "        '''\n",
    "\n",
    "        questions = {}\n",
    "        answers = {}\n",
    "        bad_sent = []\n",
    "\n",
    "        for sid, sent_id in tqdm(enumerate(table[0])):\n",
    "            question = self.create_sentance(table[1][sid], build_voc)\n",
    "            answer = self.create_sentance(table[2][sid], build_voc)\n",
    "\n",
    "            if question != None and answer != None and len(question) > 0 and len(answer) > 0:\n",
    "                questions[sent_id] = question\n",
    "                answers[sent_id] = answer\n",
    "            else:\n",
    "                bad_sent += [sid]\n",
    "\n",
    "            if build_voc:\n",
    "                _ = [self.create_sentance(table[i][sid], build_voc) for i in [3,4,5]]\n",
    "                \n",
    "        return questions, answers, bad_sent\n",
    "    \n",
    "    def clean(self, sentence, frequency):\n",
    "        ''' \n",
    "        Удаляем все редкие слова из токенизированного предложения\n",
    "        Input:\n",
    "            * sentence\n",
    "            * frequency of a word\n",
    "        Output:\n",
    "            * new sentence\n",
    "        '''\n",
    "        new_sentence = [word for word in sentence if self.words_count_dict[word] > frequency]\n",
    "        return new_sentence\n",
    "\n",
    "    def clean_all(self, questions, answers, frequency = 5):\n",
    "        ''' \n",
    "        Удаляем редкие слова из всех токенизированных пар вопрос/ответ \n",
    "        Input:\n",
    "            * questions\n",
    "            * answers\n",
    "            * frequency of a word\n",
    "        Output:\n",
    "            * amount of lost sentances\n",
    "            * new questions\n",
    "            * new answers\n",
    "        '''\n",
    "        lost_count = 0\n",
    "        new_q = {}\n",
    "        new_a = {}\n",
    "        words_counter = 0\n",
    "        for it, sent in questions.items():\n",
    "            q = self.clean(sent, frequency)\n",
    "            a = self.clean(answers[it], frequency)\n",
    "            if len(q) > 0 and len(a) > 0:\n",
    "                new_q[it] = q\n",
    "                new_a[it] = a\n",
    "                words_counter += len(q) + len(a)\n",
    "            else:\n",
    "                lost_count += 1\n",
    "        return new_q, new_a, lost_count, words_counter \n",
    "    \n",
    "    def tokenize(self, sentance, frequency = 5):\n",
    "        ''' \n",
    "        Токенизируем новое предложение\n",
    "        Input:\n",
    "            * sentance to tokenize\n",
    "            * frequency of words\n",
    "        Output:\n",
    "            * tokens(list)\n",
    "        '''\n",
    "        sent = self.create_sentance(sentance)\n",
    "        return self.clean(sent, frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:06, 1565.06it/s]\n"
     ]
    }
   ],
   "source": [
    "one_hot = one_hot_tokenizer()\n",
    "questions, answers, bad_sent = one_hot.fit(table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Посмотрим какие предложения потеряли."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Плохих сообщений : 717\n",
      "Примеры: \n",
      "q =  problema s tormozami\n",
      "a =  Поменяйте колодки, и залейте жидкость, или прокачайте тормоза, и всё.\n",
      "__________________\n",
      "q =  EH BU DUNJA  SEN NIJE BELESEN?\n",
      "a =  Фамилия наша, а гонит какой-то бред...\n",
      "__________________\n",
      "q =  Chelovek! Kto ty? Otkuda prishel? Kuda idesh'?\n",
      "a =  Я пршел с почты@mail.ru и иду на overclockers.ru\n",
      "__________________\n",
      "q =  Quosque tandem abutere, Catilina, patientia nostra?\n",
      "a =  Quam diu etiam furor iste nos eludet, quem ad finem sese effrenata iactabit audacia?  Cicerón, In Catilinam, I, I, 1\n",
      "__________________\n",
      "q =  pochemu muzhchiny zhenyatsya ne po lyubvi?\n",
      "a =  Потому, что у женщин часто бывает прошлое - а каждый мужчина хочет быть первым у женщины.  А от Вас ,наверное, ушел молодой человек... И женился на другой? Сочувствую...\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "# посмотрим сколько и чего потеряли\n",
    "def loss_stat(bad_sent, amount = 5):\n",
    "    '''\n",
    "    Input:\n",
    "        * bad_sent\n",
    "        * amount : of results        \n",
    "    '''\n",
    "    \n",
    "    print \"Плохих сообщений :\", len(bad_sent)\n",
    "    print \"Примеры: \"\n",
    "    if len(bad_sent) > amount:\n",
    "        sentances = np.random.choice(np.array(bad_sent), amount)\n",
    "    else:\n",
    "        sentances = bad_sent\n",
    "    \n",
    "    for i in sentances:\n",
    "        print \"q = \", table[1][i]\n",
    "        print \"a = \", table[2][i]\n",
    "        print \"__________________\"\n",
    "        \n",
    "loss_stat(bad_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что дейтсительно встречается мусор:  транслит, пустые ответы, смайлы, ссылки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Посмотрим полученный словарь, удалим из него редкие слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего использовано слов:  364365\n",
      "Размер словаря:  65996\n",
      "Без редких слов (реже 5): 8175\n",
      "\n",
      "TOP 10:\n",
      "это 5607\n",
      "если 4584\n",
      "есть 2774\n",
      "можно 2698\n",
      "или 2663\n",
      "только 1939\n",
      "просто 1664\n",
      "надо 1640\n",
      "очень 1607\n",
      "тебе 1594\n"
     ]
    }
   ],
   "source": [
    "# посмотрим словарь без редких слов\n",
    "def TOP(tokenizer, frequency = 5, amount = 10):\n",
    "    print \"Всего использовано слов: \", tokenizer.words_counter\n",
    "    print \"Размер словаря: \", len(tokenizer.vocabulary)\n",
    "    \n",
    "    voc = [(tokenizer.back_vocabulary[word_id], tokenizer.words_count_dict[word_id]) \n",
    "           for word_id in tokenizer.back_vocabulary\n",
    "           if tokenizer.words_count_dict[word_id] > frequency]\n",
    "    \n",
    "    voc_top = sorted(voc, key = lambda x: x[1], reverse=True)[:amount]\n",
    "    print \"Без редких слов (реже {0}): {1}\\n\".format(frequency, len(voc))\n",
    "    print \"TOP {0}:\".format(amount)\n",
    "    for i in voc_top:\n",
    "        print i[0],i[1]\n",
    "    \n",
    "TOP(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заметно:\n",
    "1. огромное количество редких слов;\n",
    "2. большую часть топа составляют stop-слова. \n",
    "\n",
    "Увы, stop-words из NTLK - не идеален. \n",
    "\n",
    "Посмотрим, что останется, если удалить редкие слова из реальных данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions, answers, lost, words_count = one_hot.clean_all(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего было: 364365 слов;\n",
      "Осталось: 126964 слов\n",
      "Всего потеряно: 237401 слов\n",
      "Всего потеряно: 350 предложений\n"
     ]
    }
   ],
   "source": [
    "def full_stat(tokenizer, lost, words_count):\n",
    "    print \"Всего было: {0} слов;\".format(tokenizer.words_counter)    \n",
    "    print \"Осталось: {0} слов\".format(words_count)\n",
    "    print \"Всего потеряно: {0} слов\".format(tokenizer.words_counter - words_count)\n",
    "    print \"Всего потеряно: {0} предложений\".format(lost)\n",
    "\n",
    "full_stat(one_hot, lost, words_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, у нас встречаются предложения, состоящие исключительно из стоп слов.\n",
    "\n",
    "Так же большинство слов теряется.\n",
    "\n",
    "Посмотрим, как работает наш токенизатор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[132, 6800, 12169]\n"
     ]
    }
   ],
   "source": [
    "sentance = u\"Привет! Как пройти до магазина?\"\n",
    "print one_hot.tokenize(sentance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Остаётся проблема: размерность словаря (= размерность one-hot вектора)\n",
    "\n",
    "Попробуем использовать стемминг для чистки слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [02:04, 80.29it/s]\n"
     ]
    }
   ],
   "source": [
    "one_hot_stem = one_hot_tokenizer(is_stem=True)\n",
    "questions_stem, answers_stem, bad_sent_stem = one_hot_stem.fit(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Плохих сообщений : 717\n",
      "Примеры: \n",
      "q =  chto mne delat ja lublu svoego jenatogo nachaljnika\n",
      "a =  А он тебя любит?? ?  \n",
      "__________________\n",
      "q =  est novost! poprosil cho bi ya dala miach. Chto delat dalshe?\n",
      "a =  Расскажи стишок:  Это был соседский мяч!\n",
      "__________________\n",
      "q =  Где найти красивую девушку со знанием английского? \n",
      "a =  Ya podoidu? Pravda ya seichas v Amerike!\n",
      "__________________\n",
      "q =  on neznaet menia ia bous! Pomogiiiiiiiiiiiiite mne on samii luchshii v mire moi kotik!!!!!!!!!!!!!\n",
      "a =  Я тебе уже писала, подойди и познакомся или напиши записку или позвони или через подруг\n",
      "__________________\n",
      "q =  где страничка с анализом динамики валют, которая была всегда доступна раньше по нажатию ссылки \"катировки\"?\n",
      "a =  None\n",
      "__________________\n"
     ]
    }
   ],
   "source": [
    "loss_stat(bad_sent_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего использовано слов:  295038\n",
      "Размер словаря:  32133\n",
      "Без редких слов (реже 5): 6327\n",
      "\n",
      "TOP 10:\n",
      "прост 1940\n",
      "очен 1609\n",
      "люб 1457\n",
      "котор 1432\n",
      "вопрос 1358\n",
      "нужн 1280\n",
      "человек 1234\n",
      "дела 1206\n",
      "жизн 1109\n",
      "поч 1091\n"
     ]
    }
   ],
   "source": [
    "TOP(one_hot_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Заметно, что топ-слова стали более смысловыми."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "questions_stem, answers_stem, lost_stem, words_count_stem = one_hot_stem.clean_all(questions_stem, answers_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Всего было: 295038 слов;\n",
      "Осталось: 119699 слов\n",
      "Всего потеряно: 175339 слов\n",
      "Всего потеряно: 193 предложений\n"
     ]
    }
   ],
   "source": [
    "full_stat(one_hot_stem, lost_stem, words_count_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Размер словаря удалось сократить, но не значительно.\n",
    "\n",
    "Так же из-за стемминга теряется смысл некоторых слов, что портит восприимчивость модели."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Векторизация слов с помощью word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем токенизировать данные с помощью модели word2vec из библиотеки gensim.\n",
    "\n",
    "Так же, для нормализации слов используем библиотеку Mystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "from gensim.models import KeyedVectors as Word2Vec\n",
    "from collections import defaultdic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее будем использовать word2vec, обученный на веб-корпусе.\n",
    "\n",
    "(источник: http://rusvectores.org/ru/models/#ruwikiruscorpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load_word2vec_format(\"web.bin\", binary=True)\n",
    "normalizer = Mystem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как работает нормализатор "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "красный\n",
      "A=вин,ед,полн,жен\n"
     ]
    }
   ],
   "source": [
    "ww = normalizer.analyze(\"красную\")[0][u'analysis'][0]\n",
    "for i in ww:\n",
    "    print ww[i] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "###### Выделим члены предложения, использованные в нашей модели word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 353608/353608 [00:00<00:00, 399711.48it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'S', 155983),\n",
       " (u'UNKN', 132466),\n",
       " (u'A', 35391),\n",
       " (u'V', 21982),\n",
       " (u'ADV', 6925),\n",
       " (u'INTJ', 161),\n",
       " (u'COM', 152),\n",
       " (u'PR', 110),\n",
       " (u'ANUM', 87),\n",
       " (u'PART', 85),\n",
       " (u'ADVPRO', 72),\n",
       " (u'CONJ', 62),\n",
       " (u'NUM', 56),\n",
       " (u'APRO', 42),\n",
       " (u'SPRO', 34)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd = defaultdict(lambda:0)\n",
    "for k in tqdm(w2v.vocab):\n",
    "    w = k.split(\"_\")\n",
    "    dd[w[1]] += 1\n",
    "sorted(dd.items(), key = lambda x: x[1], reverse= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Теперь мы можем легко приводить слова к виду word2vec, нормализуя слова и определяя члены предложения с помощью Mystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tow2v(word):\n",
    "    ''' приведём слово к виду word2vec '''\n",
    "    try:\n",
    "        word = word.encode('utf-8')\n",
    "    except:\n",
    "        pass\n",
    "    res = []\n",
    "    for x in normalizer.analyze(word):\n",
    "        if ('analysis' in x) and len(x['analysis']) > 0:\n",
    "            txt = x['analysis'][0]['lex']\n",
    "            tag = x['analysis'][0]['gr'].split(\"=\")[0].split(\",\")[0]\n",
    "            res.append(u\"{0}_{1}\".format(txt, tag))\n",
    "    return res\n",
    "\n",
    "def vectorize(text):\n",
    "    ''' векторизуем предложение '''\n",
    "    text = tow2v(text)\n",
    "    if len(text) > 0:\n",
    "        for i in text:\n",
    "            print u\"Слово {0} есть в w2v: {1}\".format(i,i in w2v.vocab)\n",
    "        return [w2v.word_vec(word) for word in text if word in w2v.vocab]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_word(word):\n",
    "    if len(word.split(' ')) > 1 or len(word) < 2:\n",
    "        return word\n",
    "\n",
    "    norm = normalizer.analyze(word)[0]\n",
    "    if 'analysis' in norm and len(norm['analysis']) > 0:\n",
    "        return norm['analysis'][0]['lex']  \n",
    "    else:\n",
    "        if 'text' in norm:\n",
    "            return norm['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Слово полет_S есть в w2v: True\n",
      "Слово кукушка_S есть в w2v: True\n",
      "Слово в_PR есть в w2v: False\n",
      "Слово чебурашка_S есть в w2v: True\n",
      "Длина полученного предложения: 3\n"
     ]
    }
   ],
   "source": [
    "print \"Длина полученного предложения: {0}\".format(len(vectorize('полёт кукушки в чебурашкой')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина вектора в нашем случае всегда будет фиксирована:  500\n"
     ]
    }
   ],
   "source": [
    "print u\"Длина вектора в нашем случае всегда будет фиксирована: \", w2v.vector_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Результаты уже не могут не радовать! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Поиск синонимов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако: задавая вопросы, люди часто используют не одни и те же слова, а заменяют их на синонимы. Поэтому, попробуем сделать поиск синонимов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В интернете удалось обнаружить несколько неплохих словарей: \n",
    "1. Контекстный, обученный на интернет ресурсах (имеются синонимы для фраз)\n",
    "2. Литературный."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# построим словарь синонимов\n",
    "class synonims():\n",
    "    ''' Построение словаря синонимов '''\n",
    "    \n",
    "    def __init__(self, w2v_file = \"web.bin\"):\n",
    "        '''\n",
    "        Input:\n",
    "            * w2v_file : word2vec model file\n",
    "        '''\n",
    "        self.rFilter = re.compile(u\"\"\"[\\'\\.\\,\\!\\\"\\№\\;\\%\\:\\?\\@\\$\\^\\*\\&\\(\\)\\_\\+\\~]\"\"\")\n",
    "        self.spaceFilter = re.compile(u'\\s+')\n",
    "        self.w2v = Word2Vec.load_word2vec_format(w2v_file, binary=True)\n",
    "        self.normalizer = Mystem()\n",
    "        self.synonims_dict = None\n",
    "        \n",
    "    def load(self, filename = \"synonims.json\"):\n",
    "        '''\n",
    "        Загружаем словарь из файла\n",
    "        Input:\n",
    "            * filename\n",
    "        '''\n",
    "        with open(filename, 'r') as fp:\n",
    "            self.synonims_dict = json.load(fp)\n",
    "    \n",
    "    def save(self, filename = \"synonims.json\"):\n",
    "        '''\n",
    "        Сохраняем словарь в файл\n",
    "        Input:\n",
    "            * filename\n",
    "        '''\n",
    "        with open(filename, 'w') as fp:\n",
    "            json.dump(self.synonims_dict, fp)\n",
    "        \n",
    "    def create(self, \n",
    "               filename = \"synmaster.txt\", \n",
    "               code = 'CP1251', \n",
    "               spliter = \"|\",\n",
    "               subspliter = None,\n",
    "               max_syn_len = 20,\n",
    "               save = True,\n",
    "               save_filename =\"synonims.json\"\n",
    "              ):\n",
    "        '''\n",
    "        Создаём/дополняем словарь используя готовый файл\n",
    "        слово синоним1 синоним2 ... синонимN\n",
    "        Input:\n",
    "            * filename\n",
    "            * code of the file\n",
    "            * spliter : word | synonims\n",
    "            * subspliter : spliter between synonims\n",
    "            * max_syn_len : maximum amount of synonims\n",
    "            * save : to save results\n",
    "            * save_filename : where to save results\n",
    "        '''\n",
    "        \n",
    "        with open(filename, \"r\") as f:\n",
    "            text = f.read()\n",
    "            text = text.decode(code)\n",
    "\n",
    "        text = text.split(\"\\n\")\n",
    "        synonims = defaultdict(lambda: list())\n",
    "        \n",
    "        for line in tqdm(text):\n",
    "            syns = line.split(spliter)\n",
    "            if subspliter is not None:\n",
    "                try:\n",
    "                    syns = [syns[0]] + syns[1].split(subspliter)\n",
    "                except:\n",
    "                    continue\n",
    "            key = self.clean(syns[0])\n",
    "            key = self.normalize_word(key)\n",
    "            is_good_key = False\n",
    "\n",
    "            if len(key) > 2:\n",
    "                if len(key.split(\" \")) < 2:\n",
    "                    vect_key = self.tow2v(key) \n",
    "                    if len(vect_key) > 0:\n",
    "                        vect_key = vect_key[0]\n",
    "                        is_good_key = True\n",
    "\n",
    "                for val in syns[1:]:\n",
    "                    val = self.clean(val)\n",
    "                    if len(val) >= 2:\n",
    "\n",
    "                        if len(val.split(\" \")) < 2:\n",
    "                            val = self.normalize_word(val)\n",
    "                            vect_val = self.tow2v(val) \n",
    "\n",
    "                            if len(vect_val) != 0 and (vect_val[0]  in self.w2v.vocab):    \n",
    "                                vect_val = vect_val[0]\n",
    "                                if vect_val not in synonims[key]:\n",
    "                                    synonims[key].append(vect_val)\n",
    "\n",
    "                        if is_good_key and vect_key not in synonims[val]:\n",
    "                            synonims[val].append(vect_key) \n",
    "        \n",
    "        print u\"Всего получено {0} слов из {1}\".format(len(synonims), filename)\n",
    "        \n",
    "        if self.synonims_dict is not None:\n",
    "            self.join(synonims)\n",
    "        else:\n",
    "            self.synonims_dict = dict(synonims)\n",
    "        \n",
    "        if save:\n",
    "            self.save(save_filename)\n",
    "    \n",
    "    def join(self, new_dict):\n",
    "        '''\n",
    "        Объединяем уже имеющийся словарь с новым\n",
    "        Input:\n",
    "            * new_dict\n",
    "        '''\n",
    "        for w in new_dict:\n",
    "            if w not in self.synonims_dict:\n",
    "                self.synonims_dict[w] = new_dict[w]\n",
    "            else:\n",
    "                for val in new_dict[w]:\n",
    "                    if val not in self.synonims_dict[w]:\n",
    "                        self.synonims_dict[w].append(val)\n",
    "    \n",
    "    def clean_base(self, max_syn_len = 20):\n",
    "        ''' \n",
    "        Обрезаем количество синонимов для каждого слова \n",
    "        Input:\n",
    "            * max_syn_len : maximum amount of synonims\n",
    "        '''\n",
    "        synonims = {}\n",
    "        for word in tqdm(self.synonims_dict):\n",
    "            if len(word.split(\" \")) < 2:\n",
    "                tok_vec = self.tow2v(word)\n",
    "                if len(tok_vec) > 0 and tok_vec[0] in self.w2v.vocab:\n",
    "                    syns = [(s, self.w2v.similarity(s, tok_vec[0])) for s in self.synonims_dict[word] if s in self.w2v.vocab]\n",
    "                    sort_list = sorted(syns, key = lambda x: x[1], reverse=True)\n",
    "                    synonims[word] = [w for w, _ in sort_list]\n",
    "                else:\n",
    "                    synonims[word] = self.synonims_dict[word]\n",
    "            else:\n",
    "                synonims[word] = self.synonims_dict[word]\n",
    "                \n",
    "            synonims[word] = synonims[word][:max_syn_len]\n",
    "            #print len(self.synonims_dict[word]), len(synonims[word])\n",
    "\n",
    "        print u\"Всего было: {0} слов\".format(len(self.synonims_dict))\n",
    "        self.synonims_dict = synonims\n",
    "        print u\"Стало: {0} слов\".format(len(self.synonims_dict))\n",
    "\n",
    "        \n",
    "    def clean(self, text):\n",
    "        ''' \n",
    "        Очищаем пришедший текст \n",
    "        Input:\n",
    "            * text\n",
    "        '''\n",
    "        text = text.lower()\n",
    "        text = self.rFilter.sub('',text)\n",
    "        text = self.spaceFilter.sub(' ', text)\n",
    "        return text.strip()\n",
    "    \n",
    "    def tow2v(self, word):\n",
    "        ''' \n",
    "        Приведём слово к виду word2vec \n",
    "        Input:\n",
    "            * word\n",
    "        '''\n",
    "        try:\n",
    "            word = word.encode('utf-8')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        res = []\n",
    "        for x in self.normalizer.analyze(word):\n",
    "            if ('analysis' in x) and len(x['analysis']) > 0:\n",
    "                txt = x['analysis'][0]['lex']\n",
    "                tag = x['analysis'][0]['gr'].split(\"=\")[0].split(\",\")[0]\n",
    "                res.append(u\"{0}_{1}\".format(txt, tag))\n",
    "        return res\n",
    "\n",
    "    def normalize_word(self, word):\n",
    "        '''\n",
    "        Нормализуем слово\n",
    "        Input:\n",
    "            * word\n",
    "        \n",
    "        '''\n",
    "        if len(word.split(' ')) > 1 or len(word) < 2:\n",
    "            return word\n",
    "\n",
    "        norm = self.normalizer.analyze(word)[0]\n",
    "        if 'analysis' in norm and len(norm['analysis']) > 0:\n",
    "            return norm['analysis'][0]['lex']  \n",
    "        else:\n",
    "            if 'text' in norm:\n",
    "                return norm['text']\n",
    "\n",
    "    def find_synonim(self, text):\n",
    "        '''\n",
    "        Поиск синонимов для всех слов в тексте\n",
    "        Input:\n",
    "            * text\n",
    "        Output:\n",
    "            * dict of sinonims\n",
    "        '''\n",
    "        syns = {}\n",
    "        tokens = self.create_synonim_tokens(text)\n",
    "        for tok in tokens:\n",
    "            if tok in self.synonims_dict:\n",
    "                syns[tok] =  self.synonims_dict[tok]\n",
    "        return syns\n",
    "    \n",
    "    def create_synonim_tokens(self, text):\n",
    "        '''\n",
    "        Разбиваем текст на слова и биграмы \n",
    "        Input:\n",
    "            * text\n",
    "        Output:\n",
    "            * list of words and bigrams\n",
    "        '''\n",
    "        try:\n",
    "            text = text.decode('utf-8')\n",
    "        except:\n",
    "            pass\n",
    "        text = self.clean(text)\n",
    "        text = text.split(' ')\n",
    "        tokens = [self.normalize_word(word) for word in text]\n",
    "        bis = []\n",
    "        for word_ind in xrange(len(text) - 1):\n",
    "            bis.append(u\" \".join([text[word_ind], text[word_ind + 1]]))\n",
    "        return tokens + bis\n",
    "    \n",
    "    def print_synonims(self, word):\n",
    "        '''\n",
    "        Вывести список синонимов для слова\n",
    "        Input:\n",
    "            * word\n",
    "        '''\n",
    "        \n",
    "        syns = self.find_synonim(word)\n",
    "        for w in syns:\n",
    "            print \"________\\nSynonims for: \", w, \" :\"\n",
    "            for j in syns[w]:\n",
    "                print \" > \", j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, что получилось:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "syns = synonims()\n",
    "syns.load(\"1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "Synonims for:  праздник  :\n",
      " >  торжество_S\n",
      " >  празднество_S\n",
      " >  празднование_S\n",
      " >  масленица_S\n",
      " >  пасха_S\n",
      " >  юбилей_S\n",
      " >  рождество_S\n",
      " >  праздничек_S\n",
      " >  именины_S\n",
      " >  гуляние_S\n",
      " >  крестины_S\n",
      " >  карнавал_S\n",
      " >  сабантуй_S\n",
      " >  курбан-байрам_S\n",
      " >  благовещение_S\n",
      " >  крещение_S\n",
      " >  сретение_S\n",
      " >  комоедица_S\n",
      " >  веселие_S\n",
      " >  вечер_S\n"
     ]
    }
   ],
   "source": [
    "syns.print_synonims(\"Праздник\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________\n",
      "Synonims for:  разбивать  :\n",
      " >  сломать_V\n",
      " >  разламывать_V\n",
      " >  разнести_V\n",
      " >  поломать_V\n",
      " >  расколачивать_V\n",
      " >  проламывать_V\n",
      " >  раздроблять_V\n",
      " >  разделять_V\n",
      " >  ломать_V\n",
      " >  поделить_V\n",
      " >  разгромить_V\n",
      " >  раскалывать_V\n",
      " >  расшибать_V\n",
      " >  сокрушать_V\n",
      " >  крушить_V\n",
      " >  разрушать_V\n",
      " >  раздолбать_V\n",
      " >  размозжить_V\n",
      " >  битый_A\n",
      " >  повреждать_V\n",
      "________\n",
      "Synonims for:  слово  :\n",
      " >  фраза_S\n",
      " >  словечко_S\n",
      " >  лексема_S\n",
      " >  термин_S\n",
      " >  выражение_S\n",
      " >  этимон_S\n",
      " >  словоформа_S\n",
      " >  антоним_S\n",
      " >  омоним_S\n",
      " >  говорить_V\n",
      " >  словцо_S\n",
      " >  изречение_S\n",
      " >  существительное_S\n",
      " >  речение_S\n",
      " >  речь_S\n",
      " >  омограф_S\n",
      " >  прилагательное_S\n",
      " >  обещание_S\n",
      " >  глагол_S\n",
      " >  пароним_S\n",
      "________\n",
      "Synonims for:  обход  :\n",
      " >  объезд_S\n",
      " >  патрулирование_S\n",
      " >  осмотр_S\n",
      " >  отвод_S\n",
      " >  огибание_S\n",
      " >  дозор_S\n",
      " >  осторожный_A\n",
      "________\n",
      "Synonims for:  текст  :\n",
      " >  шрифт_S\n",
      " >  текстовка_S\n",
      " >  слово_S\n",
      " >  стенограмма_S\n",
      " >  машинопись_S\n",
      " >  формулировка_S\n",
      " >  надпись_S\n",
      " >  аудиотекст_S\n",
      " >  словом_ADV\n",
      " >  микротекст_S\n",
      " >  гипертекст_S\n",
      " >  документ_S\n",
      " >  стих_S\n",
      " >  телетекст_S\n",
      " >  сноска_S\n",
      " >  контент_S\n",
      " >  молитва_S\n",
      " >  конферанс_S\n",
      " >  экспликация_S\n",
      " >  авест_S\n"
     ]
    }
   ],
   "source": [
    "syns.print_synonims(\"разбиваем текст <> слова и @!    биграмы    обход\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Видно, что результаты получаются очень неплохими. Можно приступать к подготовке к обучению модели."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
