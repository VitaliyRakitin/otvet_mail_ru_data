{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors as Word2Vec\n",
    "from pymystem3 import Mystem\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import sqlite3 as db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mydb = db.connect(\"part.sqlite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v = Word2Vec.load_word2vec_format(\"w2v.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mystem = Mystem()\n",
    "def tow2v(s):\n",
    "    if s is None:\n",
    "        return []\n",
    "    res = []\n",
    "    for x in mystem.analyze(s):\n",
    "        if ('analysis' in x) and len(x['analysis']) > 0:\n",
    "            txt = x['analysis'][0]['lex']\n",
    "            tag = x['analysis'][0]['gr'].split(\"=\")[0].split(\",\")[0]\n",
    "            res.append(txt + \"_\" + tag)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, batch_size = 50, n_layers=1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size\n",
    "        self.linear = nn.Linear(input_size, hidden_size).cuda()\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size).cuda()\n",
    "        \n",
    "    def forward(self, input, hidden, x):\n",
    "        output = self.linear(input).view( 1, self.batch_size, 256)\n",
    "        output, hidden1 = self.gru(output, hidden)\n",
    "        hidden = hidden1 * x + hidden1 * (1 - x)\n",
    "        return output, hidden\n",
    "\n",
    "\n",
    "            \n",
    "class SigmoidOut(nn.Module):\n",
    "    def __init__(self, input_size, batch_size=50,n_layers=1):\n",
    "        super(SigmoidOut, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.linear = nn.Linear(input_size, input_size).cuda()\n",
    "        self.sig = nn.Sigmoid().cuda()\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def forward(self, input_a, input_q):\n",
    "        input_a = input_a.view(self.batch_size,256)\n",
    "        input_q = input_q.view(self.batch_size,256)\n",
    "\n",
    "        output = self.linear(input_a)\n",
    "        output = (output * input_q).cumsum(dim = 1)[:,-1]\n",
    "        return self.sig(output)\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# если считаем заново\n",
    "batch_size = 256\n",
    "encoder_a = EncoderRNN(500, 256, batch_size).cuda()\n",
    "encoder_q = EncoderRNN(500, 256, batch_size).cuda()\n",
    "sig = SigmoidOut(256, batch_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loaded = torch.load('nn_saved/998145.tar')\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_a.load_state_dict(loaded['enc_a'])\n",
    "encoder_q.load_state_dict(loaded['enc_q'])\n",
    "sig.load_state_dict(loaded['sig'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Обучаем кодирванию предложния\n",
    "#Тут же формируется батч из предложений\n",
    "def train_sent(sent,\n",
    "               hidden,\n",
    "               encoder, \n",
    "               batch_size = 50,\n",
    "               max_len_words = 50\n",
    "              ):\n",
    "    \n",
    "    max_len = 0\n",
    "    tokens = np.zeros((max_len_words,batch_size,500), dtype = np.float) \n",
    "    x = np.zeros((max_len_words,batch_size))\n",
    "    for i, line in enumerate(sent):\n",
    "        sentance = tow2v(line)[:max_len_words]\n",
    "        j = 0\n",
    "        for word in sentance:\n",
    "            if word in w2v.vocab:\n",
    "                t = np.array(w2v.word_vec(word), dtype = np.float)\n",
    "                tokens[j, i] = t\n",
    "                x[j, i]= 1\n",
    "                j += 1\n",
    "        \n",
    "        max_len = max(j, max_len) \n",
    "        \n",
    "    input_tokens = Variable(torch.cuda.FloatTensor(np.zeros((batch_size, 500))))\n",
    "    input_x = Variable(torch.cuda.FloatTensor(np.zeros(batch_size)))\n",
    "    for i in xrange(max_len):\n",
    "        input_tokens.data = torch.cuda.FloatTensor(tokens[i])\n",
    "        input_x.data = torch.cuda.FloatTensor(x[i]).view(batch_size,1).repeat(1,256)\n",
    "        to_del, hidden = encoder(input_tokens, hidden, input_x)\n",
    "        \n",
    "    \n",
    "    return hidden\n",
    "\n",
    "#Делаем шаг обучения батча из пар вопрос ответ\n",
    "def train_step(a, \n",
    "               q, \n",
    "               target,\n",
    "               encoder_a, \n",
    "               encoder_q, \n",
    "               sig, \n",
    "               encoder_a_optimizer, \n",
    "               encoder_q_optimizer, \n",
    "               sig_optimizer, \n",
    "               criterion,\n",
    "               batch_size = 50,\n",
    "               max_len_words = 50\n",
    "              ):\n",
    "\n",
    "    if q == None or a == None:\n",
    "        return None\n",
    "    \n",
    "    hidden_a = Variable(torch.cuda.FloatTensor(np.zeros((batch_size, 256)))).view(1, batch_size,  -1)\n",
    "    hidden_q = Variable(torch.cuda.FloatTensor(np.zeros((batch_size, 256)))).view(1, batch_size,  -1)\n",
    "\n",
    "\n",
    "    encoder_a.zero_grad()\n",
    "    encoder_q.zero_grad()\n",
    "    sig.zero_grad()\n",
    "\n",
    "    hidden_q = train_sent(q, hidden_q, encoder_q, batch_size, max_len_words)\n",
    "    hidden_a = train_sent(a, hidden_a, encoder_a, batch_size, max_len_words)\n",
    "\n",
    "    \n",
    "    loss = criterion(sig(hidden_a, hidden_q), target)\n",
    "    loss.backward()\n",
    "    \n",
    "    encoder_a_optimizer.step()\n",
    "    encoder_q_optimizer.step()\n",
    "    sig_optimizer.step()\n",
    "    res = loss.data[0]\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.21 s, sys: 300 ms, total: 2.51 s\n",
      "Wall time: 2.51 s\n"
     ]
    }
   ],
   "source": [
    "#Достаем Часть выборки из БД\n",
    "%%time\n",
    "cur = mydb.execute('''select q, a, a2 from data limit 1000000''')\n",
    "training_pairs = cur.fetchall()\n",
    "len_data = len(training_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_file = open('loss.csv', \"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#главный цикл обучения\n",
    "#на каждом шаге мы берем подходщий вопрос и два неподходящих\n",
    "#в ходе обучения будем сохранять нейросеть\n",
    "n_epochs = 2000000\n",
    "learning_rate = 1e-4\n",
    "encoder_a_optimizer = optim.Adam(encoder_a.parameters(), lr=learning_rate)\n",
    "encoder_q_optimizer = optim.Adam(encoder_q.parameters(), lr=learning_rate)\n",
    "sig_optimizer = optim.Adam(sig.parameters(), lr=learning_rate)\n",
    "\n",
    "#training_pairs = [random.choice(tensors) for i in range(n_epochs)]\n",
    "criterion = nn.MSELoss().cuda()\n",
    "target = Variable(torch.cuda.FloatTensor(1))\n",
    "\n",
    "\n",
    "num_false = 3\n",
    "counter = 0\n",
    "\n",
    "for epoch in tqdm(xrange(1, n_epochs - batch_size, batch_size)):\n",
    "    loss_file.write(str(epoch))\n",
    "    counter += 1\n",
    "    slice = training_pairs[epoch:batch_size + epoch]\n",
    "    for i in range(1, 3):    \n",
    "        input_variable = [x[0] for x in slice]\n",
    "        target_variable = [x[i] for x in slice]\n",
    "        target.data = torch.cuda.FloatTensor(np.ones(batch_size))\n",
    "\n",
    "\n",
    "        loss = train_step(input_variable, target_variable, target,encoder_a, \n",
    "                   encoder_q, \n",
    "                   sig, \n",
    "                   encoder_a_optimizer, \n",
    "                   encoder_q_optimizer, \n",
    "                   sig_optimizer, \n",
    "                   criterion, batch_size)\n",
    "        loss_file.write(\"  \" + str(loss) + \" 1 \")\n",
    "\n",
    "    for i in range(1, 3):\n",
    "        input_variable = [x[0] for x in slice[::-1]]\n",
    "        target_variable = [x[i] for x in slice[::-1]]\n",
    "        target.data = torch.cuda.FloatTensor(np.zeros(batch_size))\n",
    "\n",
    "    \n",
    "        \n",
    "        loss = train_step(input_variable, target_variable, target,encoder_a, \n",
    "                   encoder_q, \n",
    "                   sig, \n",
    "                   encoder_a_optimizer, \n",
    "                   encoder_q_optimizer, \n",
    "                   sig_optimizer, \n",
    "                   criterion, batch_size)\n",
    "        loss_file.write(\"  \" + str(loss) + \" 0 \")\n",
    "    loss_file.write(\"\\n\")\n",
    "    if counter % 50 == 0:\n",
    "        torch.save({'epoch': epoch, \n",
    "                    'enc_q': encoder_q.state_dict(),\n",
    "                    'enc_a': encoder_a.state_dict(),\n",
    "                    'sig': sig.state_dict(),\n",
    "                   }, \n",
    "                   \"nn_saved/\" + str(epoch) + \".tar\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
